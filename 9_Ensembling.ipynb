{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9. Ensembling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sicily-F/cagedbirdID/blob/main/9_Ensembling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZhDnJZ3Ld23"
      },
      "source": [
        "# Ensemble learning and why we should use it?\n",
        "\n",
        "\n",
        "Code inspiration from here: https://medium.com/@muhmd.mustain/product-image-classification-using-ensemble-learning-e5b6d166afc2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GG17vl1DNsF"
      },
      "source": [
        "import numpy as np\t\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import PIL\n",
        "import pathlib\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.applications import DenseNet121, MobileNetV2\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3  \n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Average, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #What does this do- TF default GPU easily runs out of memory? https://stackoverflow.com/questions/57483567/tensorflow-device-cuda0-not-supported-by-xla-service-while-setting-up-xla-gpu\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huCmxj75KxAG"
      },
      "source": [
        "TRAIN_DIR = 'all_species_cropped_balanced/train'\n",
        "VAL_DIR = 'all_species_cropped_balanced/val'\n",
        "TEST_DIR= 'all_species_cropped_balanced/test'\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16  \n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "RANDOM_SEED = 1\n",
        "\n",
        "# We will use the default parameters of random erasing,from the paper: https://arxiv.org/pdf/1708.04896.pdf - using pixel-level randomisation\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=True): \n",
        "    def eraser(input_img):\n",
        "        if input_img.ndim == 3:\n",
        "            img_h, img_w, img_c = input_img.shape\n",
        "        elif input_img.ndim == 2:\n",
        "            img_h, img_w = input_img.shape\n",
        "\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            if input_img.ndim == 3:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "            if input_img.ndim == 2:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1/255,\n",
        " preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True),\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True) \n",
        "    \n",
        "train_gen = datagen.flow_from_directory(\n",
        "    TRAIN_DIR, \n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=RANDOM_SEED)\n",
        "\n",
        "testgen = ImageDataGenerator(\n",
        "    rescale=1/255)    \n",
        "    \n",
        "val_gen = testgen.flow_from_directory(\n",
        "    VAL_DIR, \n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=RANDOM_SEED)\n",
        "    \n",
        "test_gen = testgen.flow_from_directory(\n",
        "    TEST_DIR, \n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), \n",
        "    batch_size= BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    seed=RANDOM_SEED)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YRpVONGIHYY"
      },
      "source": [
        "There are two main API's in Keras (wrapped in Tensorflow), which you can use to add layers to existing, pre-trained models.These are the Sequential and Functional API's. The functional API offers more flexibility and control over the layers than the sequential API. It can be used to predict multiple outputs(i.e output layers) with multiple inputs(i.e input layers))\n",
        "Here we used the Functional API. You can read more [here](https://www.analyticsvidhya.com/blog/2021/07/understanding-sequential-vs-functional-api-in-keras/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv6UpfmdLPCO"
      },
      "source": [
        "# create the base pre-trained model1\n",
        "base_model1 = DenseNet121(weights='imagenet', include_top=False)\n",
        "x = base_model1.output\n",
        "# add a global spatial average pooling layer\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# and a prediction layer for our classes (37)\n",
        "predictions = Dense(37, activation='softmax')(x)\n",
        "# this is the model we will train\n",
        "model1 = Model(inputs=base_model1.input, outputs=predictions)\n",
        "for layer in base_model1.layers:\n",
        "    layer.trainable = False\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "\n",
        "# create the base pre-trained model2, which will serve as our fine-tuned model\n",
        "base_model2 = InceptionV3(weights='imagenet', include_top=False)\n",
        "x = base_model2.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(37, activation='softmax')(x)\n",
        "model2 = Model(inputs=base_model2.input, outputs=predictions)\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers, inspired from here https://keras.io/api/applications/\n",
        "for i, layer in enumerate(base_model2.layers):\n",
        "   print(i, layer.name)\n",
        "for layer in model2.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model2.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# create the base pre-trained model3    \n",
        "base_model3 = VGG16(weights='imagenet', include_top=False)\n",
        "x = base_model3.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(37, activation='softmax')(x)\n",
        "model3 = Model(inputs=base_model3.input, outputs=predictions)\n",
        "for layer in base_model3.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create the base pre-trained model3 \n",
        "base_model3 = MobileNetV2(weights='imagenet', include_top=False)\n",
        "x = base_model3.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(37, activation='softmax')(x)\n",
        "model3 = Model(inputs=base_model3.input, outputs=predictions)\n",
        "for layer in base_model3.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# This is the package which lets us download Google's Vision Transformer model\n",
        "# The code was inspired from this example with 11 classes: https://www.kaggle.com/raufmomin/vision-transformer-vit-fine-tuning\n",
        "from vit_keras import vit\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "base_model5 = vit.vit_b32(\n",
        "            image_size = IMAGE_SIZE,\n",
        "            activation = 'softmax',\n",
        "            pretrained = True,\n",
        "            include_top = False,\n",
        "            pretrained_top = False,\n",
        "            classes = 37)\n",
        "\n",
        "x = base_model4.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(11, activation=tfa.activations.gelu)(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(37, activation='softmax')(x)\n",
        "model4 = Model(inputs=base_model4.input, outputs=predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiPYzxJaLpKU"
      },
      "source": [
        "# This code averages all the models to create a singular 'ensemble' model\n",
        "models=[model1, model2, model3, model4]\n",
        "model_input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "model_outputs = [model(model_input) for model in models] \n",
        "ensemble_output = Average()(model_outputs) # Average the output of all models\n",
        "\n",
        "                            \n",
        "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n",
        "                            \n",
        "filepath = \"models/ensembleaugre16.h5\"  # This makes a folder in your root directory called 'models'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Stop training if there is no improvement in model for 3 consecutives epochs.\n",
        "early_stopping_monitor = EarlyStopping(patience=3) \n",
        "callbacks_list = [checkpoint, early_stopping_monitor]\n",
        "\n",
        "opt = Adam(lr=0.0001)\n",
        "ensemble_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzIzdcIDLr0V"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "history = ensemble_model.fit(\n",
        "    train_gen, # Will be faster if you don't have the augmentation,\n",
        "    steps_per_epoch = train_gen.samples // BATCH_SIZE,\n",
        "    validation_data = val_gen, \n",
        "    validation_steps = val_gen.samples // BATCH_SIZE,\n",
        "    epochs = EPOCHS,\n",
        "    shuffle = True,\n",
        "    callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA6jbVyqLw4w"
      },
      "source": [
        "test_batch_x, test_batch_y = test_gen.next()\n",
        "pred_batch = ensemble_model.predict(test_batch_x)\n",
        "\n",
        "test_labels = np.argmax(test_batch_y, axis=1)\n",
        "test_pred = np.argmax(pred_batch, axis=1)\n",
        "\n",
        "test_acc = sum(test_labels == test_pred) / len(test_labels)\n",
        "print('Accuracy: %.3f' % test_acc)\n",
        "\n",
        "\n",
        "Y_pred = ensemble_model.predict(test_gen, test_gen.samples // BATCH_SIZE+1)\n",
        "print (Y_pred)\n",
        "\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print(y_pred )\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "print(precision_score(test_gen.classes, y_pred , average=\"macro\"))\n",
        "print(recall_score(test_gen.classes, y_pred , average=\"macro\"))\n",
        "print(f1_score(test_gen.classes, y_pred , average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHklym8fGiRU"
      },
      "source": [
        "A full explanation of the code can be seen in file 8. with regards to model training and the evaluation metrics, so check that one out first!"
      ]
    }
  ]
}